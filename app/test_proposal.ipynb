{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb==0.3.29 in ./proposal-gpt/.venv/lib/python3.10/site-packages (0.3.29)\n",
      "Requirement already satisfied: pandas>=1.3 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.28 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (2.31.0)\n",
      "Collecting pydantic<2.0,>=1.9 (from chromadb==0.3.29)\n",
      "  Using cached pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (149 kB)\n",
      "Requirement already satisfied: hnswlib>=0.7 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (0.8.0)\n",
      "Requirement already satisfied: clickhouse-connect>=0.5.7 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (0.6.22)\n",
      "Requirement already satisfied: duckdb>=0.7.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (0.9.2)\n",
      "Requirement already satisfied: fastapi==0.85.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (0.85.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.24.0.post1)\n",
      "Requirement already satisfied: numpy>=1.21.6 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (1.26.2)\n",
      "Requirement already satisfied: posthog>=2.4.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (4.8.0)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (3.3.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (1.16.3)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (0.15.0)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (4.66.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from chromadb==0.3.29) (7.4.0)\n",
      "Requirement already satisfied: starlette==0.20.4 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from fastapi==0.85.1->chromadb==0.3.29) (0.20.4)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (3.7.1)\n",
      "Requirement already satisfied: certifi in ./proposal-gpt/.venv/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2023.11.17)\n",
      "Requirement already satisfied: urllib3>=1.26 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (1.26.18)\n",
      "Requirement already satisfied: pytz in ./proposal-gpt/.venv/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (2023.3.post1)\n",
      "Requirement already satisfied: zstandard in ./proposal-gpt/.venv/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (0.22.0)\n",
      "Requirement already satisfied: lz4 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.29) (4.3.2)\n",
      "Requirement already satisfied: coloredlogs in ./proposal-gpt/.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./proposal-gpt/.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.5.26)\n",
      "Requirement already satisfied: packaging in ./proposal-gpt/.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (23.2)\n",
      "Requirement already satisfied: protobuf in ./proposal-gpt/.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (4.25.1)\n",
      "Requirement already satisfied: sympy in ./proposal-gpt/.venv/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.3.29) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.29) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pandas>=1.3->chromadb==0.3.29) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.3.29) (2.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.29) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from requests>=2.28->chromadb==0.3.29) (3.6)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb==0.3.29) (0.19.4)\n",
      "Requirement already satisfied: click>=7.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.29) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (1.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (6.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.19.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (0.21.0)\n",
      "Requirement already satisfied: websockets>=10.4 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.29) (12.0)\n",
      "Requirement already satisfied: filelock in ./proposal-gpt/.venv/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.29) (2023.12.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.29) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.29) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in ./proposal-gpt/.venv/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette==0.20.4->fastapi==0.85.1->chromadb==0.3.29) (1.2.0)\n",
      "Using cached pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.5.2\n",
      "    Uninstalling pydantic-2.5.2:\n",
      "      Successfully uninstalled pydantic-2.5.2\n",
      "Successfully installed pydantic-1.10.13\n",
      "Requirement already satisfied: pyautogen~=0.2.0b4 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen[lmm]~=0.2.0b4) (0.2.2)\n",
      "Requirement already satisfied: diskcache in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (5.6.3)\n",
      "Requirement already satisfied: flaml in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (2.1.1)\n",
      "Requirement already satisfied: openai~=1.3 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.4.0)\n",
      "Requirement already satisfied: python-dotenv in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.0.0)\n",
      "Requirement already satisfied: termcolor in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (2.4.0)\n",
      "Requirement already satisfied: tiktoken in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (0.5.2)\n",
      "Requirement already satisfied: pillow in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen[lmm]~=0.2.0b4) (10.1.0)\n",
      "Requirement already satisfied: replicate in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pyautogen[lmm]~=0.2.0b4) (0.21.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (0.25.2)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.10.13)\n",
      "Requirement already satisfied: sniffio in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (4.8.0)\n",
      "Requirement already satisfied: NumPy>=1.17.0rc1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from flaml->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.26.2)\n",
      "Requirement already satisfied: packaging in ./proposal-gpt/.venv/lib/python3.10/site-packages (from replicate->pyautogen[lmm]~=0.2.0b4) (23.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from tiktoken->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from tiktoken->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (2.31.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (3.6)\n",
      "Requirement already satisfied: exceptiongroup in ./proposal-gpt/.venv/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.2.0)\n",
      "Requirement already satisfied: certifi in ./proposal-gpt/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in ./proposal-gpt/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=1.3->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->pyautogen~=0.2.0b4->pyautogen[lmm]~=0.2.0b4) (1.26.18)\n",
      "Requirement already satisfied: pydantic in ./proposal-gpt/.venv/lib/python3.10/site-packages (1.10.13)\n",
      "Collecting pydantic\n",
      "  Using cached pydantic-2.5.2-py3-none-any.whl.metadata (65 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pydantic) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pydantic) (2.14.5)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./proposal-gpt/.venv/lib/python3.10/site-packages (from pydantic) (4.8.0)\n",
      "Using cached pydantic-2.5.2-py3-none-any.whl (381 kB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 1.10.13\n",
      "    Uninstalling pydantic-1.10.13:\n",
      "      Successfully uninstalled pydantic-1.10.13\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.3.29 requires pydantic<2.0,>=1.9, but you have pydantic 2.5.2 which is incompatible.\n",
      "fastapi 0.85.1 requires pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2, but you have pydantic 2.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-2.5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb==0.3.29\n",
    "!pip install \"pyautogen[lmm]~=0.2.0b4\"\n",
    "!pip install -U pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.chdir('../')\n",
    "import autogen\n",
    "from prompts.plan_proposal import proposal_prompt\n",
    "from functions.draft_proposal import store_draft_proposal_schema, store_draft_proposal\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"/root/projects/proposal-gpt/OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The specified config_list file 'OAI_CONFIG_LIST' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "<client_domain> (Client Domain): Healthcare\n",
      "\n",
      "<client_problems_list> (Client Problems List):\n",
      "\n",
      "Inefficient patient data management and retrieval.\n",
      "High administrative workload for healthcare professionals.\n",
      "Difficulty in early disease detection and diagnosis.\n",
      "Limited access to real-time patient insights.\n",
      "Privacy and security concerns related to patient data.\n",
      "<solution> (Solution):\n",
      "\n",
      "Proposed Solution Structure:\n",
      "Component 1: Implement an AI-driven Electronic Health Record (EHR) system for efficient data management and retrieval.\n",
      "Component 2: Develop a virtual assistant to automate administrative tasks and streamline workflows for healthcare professionals.\n",
      "Component 3: Create a predictive analytics module for early disease detection and diagnosis.\n",
      "Component 4: Build a secure patient portal for real-time access to medical records and insights.\n",
      "\n",
      "<case_studies_list> (Case Studies List):\n",
      "\n",
      "Case Study 1: Enhanced Patient Care\n",
      "\n",
      "Description: Demonstrates how our AI-powered EHR system improved patient care and reduced errors in a hospital, resulting in a 20% reduction in readmissions.\n",
      "Metrics: Reduced readmissions, improved patient outcomes, healthcare provider testimonial.\n",
      "Case Study 2: Administrative Efficiency\n",
      "\n",
      "Description: Highlights how our virtual assistant reduced administrative workload for a medical practice, leading to a 30% increase in appointment scheduling efficiency.\n",
      "Metrics: Administrative time savings, appointment scheduling accuracy, client testimonial.\n",
      "Case Study 3: Disease Detection\n",
      "\n",
      "Description: Illustrates our success in early disease detection using predictive analytics, resulting in a 40% increase in early-stage diagnoses for a healthcare institution.\n",
      "Metrics: Early-stage diagnoses, patient outcomes, healthcare provider feedback.\n",
      "\n",
      "<success_stories_list> (Success Stories List):\n",
      "\n",
      "Success Story 1: Revenue Growth\n",
      "\n",
      "Description: Our AI-powered solutions contributed to a 15% increase in revenue for a healthcare client by improving patient care and operational efficiency.\n",
      "Metrics: Revenue growth, ROI, client testimonial.\n",
      "Success Story 2: Streamlined Operations\n",
      "\n",
      "Description: We helped a medical practice achieve operational efficiency, reducing administrative costs by 25% and enhancing patient satisfaction.\n",
      "Metrics: Cost savings, patient satisfaction scores, operational metrics.\n",
      "Success Story 3: Data Security\n",
      "\n",
      "Description: Our advanced security measures ensured compliance and data security for a healthcare organization, mitigating risks and enhancing trust.\n",
      "Metrics: Data security compliance, risk mitigation, client feedback.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "mode is only supported in Pydantic v2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/root/projects/proposal-gpt/app/test_proposal.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m manager \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mGroupChatManager(groupchat\u001b[39m=\u001b[39mgroupchat, llm_config\u001b[39m=\u001b[39mcommon_config)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m<client_domain> (Client Domain): Healthcare\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mMetrics: Data security compliance, risk mitigation, client feedback.\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     manager,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m )\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:556\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[1;32m    544\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 556\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    352\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    353\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 354\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:487\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    488\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:962\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 962\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    964\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/groupchat.py:340\u001b[0m, in \u001b[0;36mGroupChatManager.run_chat\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    338\u001b[0m     speaker \u001b[39m=\u001b[39m groupchat\u001b[39m.\u001b[39mselect_speaker(speaker, \u001b[39mself\u001b[39m)\n\u001b[1;32m    339\u001b[0m     \u001b[39m# let the speaker speak\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m     reply \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39;49mgenerate_reply(sender\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    341\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     \u001b[39m# let the admin agent speak if interrupted\u001b[39;00m\n\u001b[1;32m    343\u001b[0m     \u001b[39mif\u001b[39;00m groupchat\u001b[39m.\u001b[39madmin_name \u001b[39min\u001b[39;00m groupchat\u001b[39m.\u001b[39magent_names:\n\u001b[1;32m    344\u001b[0m         \u001b[39m# admin agent is one of the participants\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:962\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 962\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    963\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    964\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:638\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    636\u001b[0m extracted_response \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39mextract_text_or_completion_object(response)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    637\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(extracted_response, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 638\u001b[0m     extracted_response \u001b[39m=\u001b[39m extracted_response\u001b[39m.\u001b[39;49mmodel_dump(mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mdict\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    639\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, extracted_response\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/openai/_models.py:190\u001b[0m, in \u001b[0;36mBaseModel.model_dump\u001b[0;34m(self, mode, include, exclude, by_alias, exclude_unset, exclude_defaults, exclude_none, round_trip, warnings)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Usage docs: https://docs.pydantic.dev/2.4/concepts/serialization/#modelmodel_dump\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[1;32m    171\u001b[0m \u001b[39mGenerate a dictionary representation of the model, optionally specifying which fields to include or exclude.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[39m    A dictionary representation of the model.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpython\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 190\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmode is only supported in Pydantic v2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    191\u001b[0m \u001b[39mif\u001b[39;00m round_trip \u001b[39m!=\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mround_trip is only supported in Pydantic v2\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: mode is only supported in Pydantic v2"
     ]
    }
   ],
   "source": [
    "\n",
    "common_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "draft_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "    \"functions\": [\n",
    "        store_draft_proposal_schema\n",
    "    ]\n",
    "}\n",
    "\n",
    "config_list_dalle = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"dalle\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"Admin\",\n",
    "   system_message=\"\"\"A human admin. Interact with the bd_draft and bd_critic to draft the proposal, bd_critic will verify and give comments to improve the draft proposal content and structure. \n",
    "   The draft of proposal needs to be approved by this admin.\n",
    "   \"\"\",\n",
    "   code_execution_config=False,\n",
    ")\n",
    "\n",
    "draft = autogen.AssistantAgent(\n",
    "    name=\"bd_draft\",\n",
    "    system_message=proposal_prompt,\n",
    "    llm_config=draft_config,\n",
    ")\n",
    "\n",
    "draft.register_function(\n",
    "    function_map={\n",
    "        \"store_draft_proposal\": store_draft_proposal,\n",
    "    }\n",
    ")\n",
    "\n",
    "critic = autogen.AssistantAgent(\n",
    "    name=\"bd_critic\",\n",
    "    system_message=\"Critic. Double check the proposal from the executer and provide feedback.\",\n",
    "    llm_config=common_config,\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, draft, critic], messages=[], max_round=50)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=common_config)\n",
    "\n",
    "input = \"\"\"\n",
    "<client_domain> (Client Domain): Healthcare\n",
    "\n",
    "<client_problems_list> (Client Problems List):\n",
    "\n",
    "Inefficient patient data management and retrieval.\n",
    "High administrative workload for healthcare professionals.\n",
    "Difficulty in early disease detection and diagnosis.\n",
    "Limited access to real-time patient insights.\n",
    "Privacy and security concerns related to patient data.\n",
    "<solution> (Solution):\n",
    "\n",
    "Proposed Solution Structure:\n",
    "Component 1: Implement an AI-driven Electronic Health Record (EHR) system for efficient data management and retrieval.\n",
    "Component 2: Develop a virtual assistant to automate administrative tasks and streamline workflows for healthcare professionals.\n",
    "Component 3: Create a predictive analytics module for early disease detection and diagnosis.\n",
    "Component 4: Build a secure patient portal for real-time access to medical records and insights.\n",
    "\n",
    "<case_studies_list> (Case Studies List):\n",
    "\n",
    "Case Study 1: Enhanced Patient Care\n",
    "\n",
    "Description: Demonstrates how our AI-powered EHR system improved patient care and reduced errors in a hospital, resulting in a 20% reduction in readmissions.\n",
    "Metrics: Reduced readmissions, improved patient outcomes, healthcare provider testimonial.\n",
    "Case Study 2: Administrative Efficiency\n",
    "\n",
    "Description: Highlights how our virtual assistant reduced administrative workload for a medical practice, leading to a 30% increase in appointment scheduling efficiency.\n",
    "Metrics: Administrative time savings, appointment scheduling accuracy, client testimonial.\n",
    "Case Study 3: Disease Detection\n",
    "\n",
    "Description: Illustrates our success in early disease detection using predictive analytics, resulting in a 40% increase in early-stage diagnoses for a healthcare institution.\n",
    "Metrics: Early-stage diagnoses, patient outcomes, healthcare provider feedback.\n",
    "\n",
    "<success_stories_list> (Success Stories List):\n",
    "\n",
    "Success Story 1: Revenue Growth\n",
    "\n",
    "Description: Our AI-powered solutions contributed to a 15% increase in revenue for a healthcare client by improving patient care and operational efficiency.\n",
    "Metrics: Revenue growth, ROI, client testimonial.\n",
    "Success Story 2: Streamlined Operations\n",
    "\n",
    "Description: We helped a medical practice achieve operational efficiency, reducing administrative costs by 25% and enhancing patient satisfaction.\n",
    "Metrics: Cost savings, patient satisfaction scores, operational metrics.\n",
    "Success Story 3: Data Security\n",
    "\n",
    "Description: Our advanced security measures ensured compliance and data security for a healthcare organization, mitigating risks and enhancing trust.\n",
    "Metrics: Data security compliance, risk mitigation, client feedback.\n",
    "\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    message=input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draft.chat_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'OpenAIWrapper' from 'autogen' (/root/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/root/projects/proposal-gpt/app/test_proposal.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mautogen\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtest_dalle\u001b[39;00m \u001b[39mimport\u001b[39;00m DALLEAgent\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magentchat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mretrieve_user_proxy_agent\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrieveUserProxyAgent\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/root/projects/proposal-gpt/app/test_proposal.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mchromadb\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/proposal-gpt/app/test_dalle.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Dict, List, Optional, Union\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mautogen\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magentchat\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent, ConversableAgent\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39magentchat\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimg_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m _to_pil\n\u001b[1;32m     13\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39murllib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrequest\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39massistant_agent\u001b[39;00m \u001b[39mimport\u001b[39;00m AssistantAgent\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconversable_agent\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversableAgent\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mgroupchat\u001b[39;00m \u001b[39mimport\u001b[39;00m GroupChat, GroupChatManager\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/assistant_agent.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Dict, Literal, Optional, Union\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mconversable_agent\u001b[39;00m \u001b[39mimport\u001b[39;00m ConversableAgent\n\u001b[1;32m      6\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mAssistantAgent\u001b[39;00m(ConversableAgent):\n\u001b[1;32m      7\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"(In preview) Assistant agent, designed to solve a task with LLM.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[39m    AssistantAgent is a subclass of ConversableAgent configured with a default system message.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m    This agent doesn't execute code by default, and expects the user to execute the code.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcollections\u001b[39;00m \u001b[39mimport\u001b[39;00m defaultdict\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Callable, Dict, List, Literal, Optional, Tuple, Type, Union\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogen\u001b[39;00m \u001b[39mimport\u001b[39;00m OpenAIWrapper\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogen\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcode_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m DEFAULT_MODEL, UNKNOWN, content_str, execute_code, extract_code, infer_lang\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39magent\u001b[39;00m \u001b[39mimport\u001b[39;00m Agent\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'OpenAIWrapper' from 'autogen' (/root/projects/proposal-gpt/.venv/lib/python3.10/site-packages/autogen/__init__.py)"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from test_dalle import DALLEAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "common_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "\n",
    "config_list_dalle = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"dalle\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "dalle_config = {\n",
    "    \"cache_seed\": 42,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_dalle,\n",
    "    \"timeout\": 120,\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"store_image_url\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"slide_title\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": (\n",
    "                            \"Enter the section title of the image.\"\n",
    "                        ),\n",
    "                    },\n",
    "                    \"url\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": (\n",
    "                            \"Enter the url of the image.\"\n",
    "                        ),\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"slide_title\",\n",
    "                    \"url\"\n",
    "                ]\n",
    "            },\n",
    "            \"description\": \"This is a function allowing image_generator to input the section title and the image url.\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def termination_msg(x): return isinstance(\n",
    "    x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "\n",
    "\n",
    "user_proxy = RetrieveUserProxyAgent(\n",
    "    name=\"user\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"Assistant who has the content of draft proposal.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"./draft_proposal.txt\",\n",
    "        \"chunk_token_size\": 1000,\n",
    "        \"model\": config_list_gpt4[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"./chromadb\"),\n",
    "        \"collection_name\": \"draft_proposal\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config=False,  # we don't want to execute code in this case.\n",
    ")\n",
    "\n",
    "image_reviewer = autogen.AssistantAgent(\n",
    "    name=\"image_reviewer\",\n",
    "    system_message=\"Image Reviewer. Double check the draft proposal from the executer and provide ideas of images for each sections. Try to provide clear description. Only response json array with each element has 2 key slide_title and image_description\",\n",
    "    llm_config=common_config,\n",
    ")\n",
    "\n",
    "image_generator = DALLEAgent(\n",
    "    name=\"image_generator\",\n",
    "    system_message=\"Image Generator. Read the list slide title and image description and generate image for each slide.\",\n",
    "    llm_config=dalle_config,\n",
    ")\n",
    "\n",
    "def store_image_url(slide_title, url):\n",
    "    print('store_image_url')\n",
    "    print((slide_title, url))\n",
    "\n",
    "image_generator.register_function(\n",
    "    function_map={\n",
    "        \"store_image_url\": store_image_url,\n",
    "    }\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, image_reviewer, image_generator], messages=[], max_round=50)\n",
    "manager = autogen.GroupChatManager(\n",
    "    groupchat=groupchat, llm_config=common_config)\n",
    "\n",
    "input_str = \"\"\"Generate images for the proposal.\"\"\"\n",
    "user_proxy.initiate_chat(manager, problem=input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sibae/miniforge3/envs/autogen/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_1', 'doc_0']]\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[33madmin\u001b[0m (to bd_final):\n",
      "\n",
      "You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "For code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "User's question is: Create final proposal with the provided content.\n",
      "\n",
      "Context is: [\n",
      "    {\n",
      "        \"slide_title\": \"Executive Summary\",\n",
      "        \"url\": \"./images/Executive Summary.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"Understanding Client Pain Points\",\n",
      "        \"url\": \"./images/Understanding Client Pain Points.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"Solution Overview\",\n",
      "        \"url\": \"./images/Solution Overview.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"How the Solution Works\",\n",
      "        \"url\": \"./images/How the Solution Works.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"Case Studies\",\n",
      "        \"url\": \"./images/Case Studies.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"Success Stories and Metrics\",\n",
      "        \"url\": \"./images/Success Stories and Metrics.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"Timeline, Costs, and Payment Terms\",\n",
      "        \"url\": \"./images/Timeline, Costs, and Payment Terms.jpg\"\n",
      "    },\n",
      "    {\n",
      "        \"slide_title\": \"Conclusion\",\n",
      "        \"url\": \"./images/Conclusion.jpg\"\n",
      "    }\n",
      "]\n",
      "**Slide 1: Executive Summary**\n",
      "\n",
      "Our proposal offers a unique, AI-powered solution that stands out in the market due to its ability to address your key challenges effectively. We aim to transform your healthcare organization by improving patient data management, reducing administrative workload, enabling early disease detection, providing real-time patient insights, and ensuring data privacy.\n",
      "\n",
      "**Slide 2: Understanding Client Pain Points**\n",
      "\n",
      "- Inefficient patient data management and retrieval\n",
      "- High administrative workload for healthcare professionals\n",
      "- Difficulty in early disease detection and diagnosis\n",
      "- Limited access to real-time patient insights\n",
      "- Privacy and security concerns related to patient data\n",
      "\n",
      "**Slide 3: Solution Overview**\n",
      "\n",
      "Our solution comprises four key components, each designed to address your specific pain points:\n",
      "\n",
      "1. AI-driven Electronic Health Record (EHR) system: Streamlines patient data management and retrieval.\n",
      "2. Virtual assistant: Reduces administrative workload by automating tasks.\n",
      "3. Predictive analytics module: Enables early disease detection and diagnosis.\n",
      "4. Secure patient portal: Provides real-time access to medical records, ensuring data privacy.\n",
      "\n",
      "**Slide 4: How the Solution Works**\n",
      "\n",
      "[Include a diagram or visual aid]\n",
      "\n",
      "1. The AI-driven EHR system organizes and manages patient data efficiently, reducing errors and improving patient care.\n",
      "2. The virtual assistant automates administrative tasks, allowing healthcare professionals to focus on patient care.\n",
      "3. The predictive analytics module uses AI to analyze patient data and detect diseases early, improving patient outcomes.\n",
      "4. The secure patient portal provides patients with real-time access to their medical records, enhancing patient engagement and satisfaction.\n",
      "\n",
      "**Slide 5: Case Studies**\n",
      "\n",
      "1. Enhanced Patient Care: Our AI-powered EHR system was implemented in a large hospital, where it reduced readmissions by 20%.\n",
      "2. Administrative Efficiency: A medium-sized medical practice used our virtual assistant, leading to a 30% increase in appointment scheduling efficiency.\n",
      "3. Disease Detection: A healthcare institution used our predictive analytics module, resulting in a 40% increase in early-stage diagnoses.\n",
      "\n",
      "**Slide 6: Success Stories and Metrics**\n",
      "\n",
      "1. Revenue Growth: Our solutions contributed to a 15% increase in revenue for a large healthcare client, resulting in a significant ROI.\n",
      "2. Streamlined Operations: We helped a medium-sized medical practice reduce administrative costs by 25%, enhancing patient satisfaction.\n",
      "3. Data Security: Our advanced security measures ensured compliance and data security for a large healthcare organization, mitigating risks and enhancing trust.\n",
      "\n",
      "**Slide 7: Timeline, Costs, and Payment Terms**\n",
      "\n",
      "We propose a phased implementation over 12 months, with costs broken down as follows:\n",
      "\n",
      "1. AI-driven EHR system: $X\n",
      "2. Virtual assistant: $Y\n",
      "3. Predictive analytics module: $Z\n",
      "4. Secure patient portal: $W\n",
      "\n",
      "Payment terms are flexible and can be discussed further.\n",
      "\n",
      "**Slide 8: Conclusion**\n",
      "\n",
      "Our unique, AI-powered solution offers significant benefits, including improved patient care, reduced administrative workload, early disease detection, real-time patient insights, and enhanced data security. We look forward to partnering with you to transform your healthcare organization.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mbd_final\u001b[0m (to admin):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: store_final_proposal *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "\"final\": \"# Executive Summary\\n\\nOur proposal offers a unique, AI-powered solution that stands out in the market due to its ability to address your key challenges effectively. We aim to transform your healthcare organization by improving patient data management, reducing administrative workload, enabling early disease detection, providing real-time patient insights, and ensuring data privacy.\\n\\n![Executive Summary](./images/Executive Summary.jpg)\\n\\n# Understanding Client Pain Points\\n\\n- Inefficient patient data management and retrieval\\n- High administrative workload for healthcare professionals\\n- Difficulty in early disease detection and diagnosis\\n- Limited access to real-time patient insights\\n- Privacy and security concerns related to patient data\\n\\n![Understanding Client Pain Points](./images/Understanding Client Pain Points.jpg)\\n\\n# Solution Overview\\n\\nOur solution comprises four key components, each designed to address your specific pain points:\\n\\n1. AI-driven Electronic Health Record (EHR) system: Streamlines patient data management and retrieval.\\n2. Virtual assistant: Reduces administrative workload by automating tasks.\\n3. Predictive analytics module: Enables early disease detection and diagnosis.\\n4. Secure patient portal: Provides real-time access to medical records, ensuring data privacy.\\n\\n![Solution Overview](./images/Solution Overview.jpg)\\n\\n# How the Solution Works\\n\\n[Include a diagram or visual aid]\\n\\n1. The AI-driven EHR system organizes and manages patient data efficiently, reducing errors and improving patient care.\\n2. The virtual assistant automates administrative tasks, allowing healthcare professionals to focus on patient care.\\n3. The predictive analytics module uses AI to analyze patient data and detect diseases early, improving patient outcomes.\\n4. The secure patient portal provides patients with real-time access to their medical records, enhancing patient engagement and satisfaction.\\n\\n![How the Solution Works](./images/How the Solution Works.jpg)\\n\\n# Case Studies\\n\\n1. Enhanced Patient Care: Our AI-powered EHR system was implemented in a large hospital, where it reduced readmissions by 20%.\\n2. Administrative Efficiency: A medium-sized medical practice used our virtual assistant, leading to a 30% increase in appointment scheduling efficiency.\\n3. Disease Detection: A healthcare institution used our predictive analytics module, resulting in a 40% increase in early-stage diagnoses.\\n\\n![Case Studies](./images/Case Studies.jpg)\\n\\n# Success Stories and Metrics\\n\\n1. Revenue Growth: Our solutions contributed to a 15% increase in revenue for a large healthcare client, resulting in a significant ROI.\\n2. Streamlined Operations: We helped a medium-sized medical practice reduce administrative costs by 25%, enhancing patient satisfaction.\\n3. Data Security: Our advanced security measures ensured compliance and data security for a large healthcare organization, mitigating risks and enhancing trust.\\n\\n![Success Stories and Metrics](./images/Success Stories and Metrics.jpg)\\n\\n# Timeline, Costs, and Payment Terms\\n\\nWe propose a phased implementation over 12 months, with costs broken down as follows:\\n\\n1. AI-driven EHR system: $X\\n2. Virtual assistant: $Y\\n3. Predictive analytics module: $Z\\n4. Secure patient portal: $W\\n\\nPayment terms are flexible and can be discussed further.\\n\\n![Timeline, Costs, and Payment Terms](./images/Timeline, Costs, and Payment Terms.jpg)\\n\\n# Conclusion\\n\\nOur unique, AI-powered solution offers significant benefits, including improved patient care, reduced administrative workload, early disease detection, real-time patient insights, and enhanced data security. We look forward to partnering with you to transform your healthcare organization.\\n\\n![Conclusion](./images/Conclusion.jpg)\"\n",
      "}\n",
      "\u001b[32m*********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 105\u001b[0m\n\u001b[1;32m     97\u001b[0m draft\u001b[38;5;241m.\u001b[39mregister_function(\n\u001b[1;32m     98\u001b[0m     function_map\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_final_proposal\u001b[39m\u001b[38;5;124m\"\u001b[39m: store_final_proposal,\n\u001b[1;32m    100\u001b[0m     }\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mCreate final proposal with the provided content.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 105\u001b[0m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproblem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:556\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;124;03m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 556\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_init_message\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    352\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 354\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:489\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    487\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:354\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    352\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 354\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    358\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:487\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 487\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:962\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m--> 962\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m    964\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:320\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent._generate_retrieve_user_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    318\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    319\u001b[0m message \u001b[38;5;241m=\u001b[39m messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 320\u001b[0m update_context_case1, update_context_case2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_update_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (update_context_case1 \u001b[38;5;129;01mor\u001b[39;00m update_context_case2) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_context:\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mprint\u001b[39m(colored(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating context and resetting conversation.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m), flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/autogen/lib/python3.10/site-packages/autogen/agentchat/contrib/retrieve_user_proxy_agent.py:300\u001b[0m, in \u001b[0;36mRetrieveUserProxyAgent._check_update_context\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    299\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 300\u001b[0m update_context_case1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUPDATE CONTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mupper() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUPDATE CONTEXT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message[:\u001b[38;5;241m20\u001b[39m]\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m    301\u001b[0m update_context_case2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustomized_answer_prefix \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustomized_answer_prefix \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39mupper()\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m update_context_case1, update_context_case2\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "from prompts.plan_proposal import proposal_prompt\n",
    "from test_dalle import DALLEAgent\n",
    "from functions.draft_proposal import store_draft_proposal_schema, store_draft_proposal\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import chromadb\n",
    "import autogen\n",
    "from prompts.plan_proposal import proposal_prompt\n",
    "from test_dalle import DALLEAgent\n",
    "from functions.draft_proposal import store_draft_proposal_schema, store_draft_proposal\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\"],\n",
    "    },\n",
    ")\n",
    "common_config = {\n",
    "    \"cache_seed\": 43,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "}\n",
    "def termination_msg(x): return isinstance(\n",
    "    x, dict) and \"TERMINATE\" == str(x.get(\"content\", \"\"))[-9:].upper()\n",
    "draft_config = {\n",
    "    \"cache_seed\": 43,  # change the cache_seed for different trials\n",
    "    \"temperature\": 0,\n",
    "    \"config_list\": config_list_gpt4,\n",
    "    \"timeout\": 120,\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"store_final_proposal\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"final\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": (\n",
    "                            \"Enter the final of proposal in markdown format.\"\n",
    "                        ),\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"final\"\n",
    "                ]\n",
    "            },\n",
    "            \"description\": \"This is a function allowing bd_final to input the final proposal to store in markdown format.\",\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "user_proxy = RetrieveUserProxyAgent(\n",
    "    name=\"admin\",\n",
    "    is_termination_msg=termination_msg,\n",
    "    system_message=\"Assistant who has the content of draft proposal and list image url.\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=1,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": [\"./draft_proposal.txt\", \"./image.json\"],\n",
    "        \"chunk_token_size\": 4000,\n",
    "        \"model\": config_list_gpt4[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"./chromadb\"),\n",
    "        \"collection_name\": \"final_proposal\",\n",
    "        \"get_or_create\": True,\n",
    "    },\n",
    "    code_execution_config=False,  # we don't want to execute code in this case.\n",
    ")\n",
    "\n",
    "draft = autogen.AssistantAgent(\n",
    "    name=\"bd_final\",\n",
    "    system_message=\"\"\"Act as a professional business development specialist in AI-powered software company.\n",
    "Your goal is creating a compelling proposal slide deck for our client.\n",
    "You will be provided draft proposal and list image and slide. \n",
    "Please ensure that each of these elements is presented in the proposal slides. Your goal is to create a persuasive and informative proposal that resonates with the client's needs and convinces them of the value we can provide.\n",
    "Finally, store the draft of proposal in text format.\"\"\",\n",
    "    llm_config=draft_config,\n",
    ")\n",
    "\n",
    "def store_final_proposal(draft):\n",
    "    \"\"\"\n",
    "    Store draft of proposal in a local file.\n",
    "    \"\"\"\n",
    "    # Exception\n",
    "    if not isinstance(draft, str):\n",
    "        raise TypeError(\"Draft of proposal must be a string.\")\n",
    "    # Function implementation...\n",
    "    with open(\"./final_proposal.md\", \"w\") as f:\n",
    "        f.write(draft)\n",
    "    # False case\n",
    "    if not os.path.exists(\"./final_proposal.md\"):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "draft.register_function(\n",
    "    function_map={\n",
    "        \"store_final_proposal\": store_final_proposal,\n",
    "    }\n",
    ")\n",
    "\n",
    "input = \"\"\"Create final proposal with the provided content.\"\"\"\n",
    "\n",
    "user_proxy.initiate_chat(\n",
    "    draft,\n",
    "    problem=input,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.a_check_termination_and_human_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[str]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.check_termination_and_human_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[str]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.a_generate_function_call_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[Dict]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.generate_function_call_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Optional[Dict]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.generate_code_execution_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Union[Dict, Literal[False], NoneType] = None)>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.a_generate_oai_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[Any] = None) -> Tuple[bool, Union[str, Dict, NoneType]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None},\n",
       " {'trigger': [autogen.agentchat.agent.Agent, None],\n",
       "  'reply_func': <function autogen.agentchat.conversable_agent.ConversableAgent.generate_oai_reply(self, messages: Optional[List[Dict]] = None, sender: Optional[autogen.agentchat.agent.Agent] = None, config: Optional[autogen.oai.client.OpenAIWrapper] = None) -> Tuple[bool, Union[str, Dict, NoneType]]>,\n",
       "  'config': None,\n",
       "  'init_config': None,\n",
       "  'reset_config': None}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draft._reply_func_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_1', 'doc_0']]\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You\\'re a retrieve augmented coding assistant. You answer user\\'s questions based on your own knowledge and the\\ncontext provided by the user.\\nIf you can\\'t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\nFor code generation, you must obey the following rules:\\nRule 1. You MUST NOT install any packages because all the packages needed are already installed.\\nRule 2. You must follow the formats below to write your code:\\n```language\\n# your code\\n```\\n\\nUser\\'s question is: Create final proposal with the provided content.\\n\\nContext is: [\\n    {\\n        \"slide_title\": \"Executive Summary\",\\n        \"url\": \"./images/Executive Summary.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"Understanding Client Pain Points\",\\n        \"url\": \"./images/Understanding Client Pain Points.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"Solution Overview\",\\n        \"url\": \"./images/Solution Overview.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"How the Solution Works\",\\n        \"url\": \"./images/How the Solution Works.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"Case Studies\",\\n        \"url\": \"./images/Case Studies.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"Success Stories and Metrics\",\\n        \"url\": \"./images/Success Stories and Metrics.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"Timeline, Costs, and Payment Terms\",\\n        \"url\": \"./images/Timeline, Costs, and Payment Terms.jpg\"\\n    },\\n    {\\n        \"slide_title\": \"Conclusion\",\\n        \"url\": \"./images/Conclusion.jpg\"\\n    }\\n]\\n**Slide 1: Executive Summary**\\n\\nOur proposal offers a unique, AI-powered solution that stands out in the market due to its ability to address your key challenges effectively. We aim to transform your healthcare organization by improving patient data management, reducing administrative workload, enabling early disease detection, providing real-time patient insights, and ensuring data privacy.\\n\\n**Slide 2: Understanding Client Pain Points**\\n\\n- Inefficient patient data management and retrieval\\n- High administrative workload for healthcare professionals\\n- Difficulty in early disease detection and diagnosis\\n- Limited access to real-time patient insights\\n- Privacy and security concerns related to patient data\\n\\n**Slide 3: Solution Overview**\\n\\nOur solution comprises four key components, each designed to address your specific pain points:\\n\\n1. AI-driven Electronic Health Record (EHR) system: Streamlines patient data management and retrieval.\\n2. Virtual assistant: Reduces administrative workload by automating tasks.\\n3. Predictive analytics module: Enables early disease detection and diagnosis.\\n4. Secure patient portal: Provides real-time access to medical records, ensuring data privacy.\\n\\n**Slide 4: How the Solution Works**\\n\\n[Include a diagram or visual aid]\\n\\n1. The AI-driven EHR system organizes and manages patient data efficiently, reducing errors and improving patient care.\\n2. The virtual assistant automates administrative tasks, allowing healthcare professionals to focus on patient care.\\n3. The predictive analytics module uses AI to analyze patient data and detect diseases early, improving patient outcomes.\\n4. The secure patient portal provides patients with real-time access to their medical records, enhancing patient engagement and satisfaction.\\n\\n**Slide 5: Case Studies**\\n\\n1. Enhanced Patient Care: Our AI-powered EHR system was implemented in a large hospital, where it reduced readmissions by 20%.\\n2. Administrative Efficiency: A medium-sized medical practice used our virtual assistant, leading to a 30% increase in appointment scheduling efficiency.\\n3. Disease Detection: A healthcare institution used our predictive analytics module, resulting in a 40% increase in early-stage diagnoses.\\n\\n**Slide 6: Success Stories and Metrics**\\n\\n1. Revenue Growth: Our solutions contributed to a 15% increase in revenue for a large healthcare client, resulting in a significant ROI.\\n2. Streamlined Operations: We helped a medium-sized medical practice reduce administrative costs by 25%, enhancing patient satisfaction.\\n3. Data Security: Our advanced security measures ensured compliance and data security for a large healthcare organization, mitigating risks and enhancing trust.\\n\\n**Slide 7: Timeline, Costs, and Payment Terms**\\n\\nWe propose a phased implementation over 12 months, with costs broken down as follows:\\n\\n1. AI-driven EHR system: $X\\n2. Virtual assistant: $Y\\n3. Predictive analytics module: $Z\\n4. Secure patient portal: $W\\n\\nPayment terms are flexible and can be discussed further.\\n\\n**Slide 8: Conclusion**\\n\\nOur unique, AI-powered solution offers significant benefits, including improved patient care, reduced administrative workload, early disease detection, real-time patient insights, and enhanced data security. We look forward to partnering with you to transform your healthcare organization.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_proxy.generate_init_message(problem=input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
